[Broken Hosts Sanity Check]
action.email = 1
action.email.format = csv
action.email.include.results_link = 0
action.email.include.search = 0
action.email.inline = 0
action.email.message.alert = The alert condition for '$name$' was triggered.\
\
$result.Message$
action.email.reportServerEnabled = 0
action.email.sendcsv = 0
action.email.sendpdf = 0
action.email.sendresults = 0
action.email.subject = Splunk Alert: $name$
action.email.to = $result.contact$
alert.digest_mode = 0
alert.suppress = 1
alert.suppress.period = 2h
alert.suppress.fields = contact,suppressfield
alert.track = 0
alert.expires = 30m
counttype = number of events
cron_schedule = */30 * * * *
description = Checks how long since we've gotten data from various hosts
disabled = 0
dispatch.earliest_time = -30d@d
dispatch.latest_time = +7d@d
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_view = search
schedule_window = 20
search = | tstats latest(_time) as lastTime latest(date_zone) as date_zone latest(date_hour) as date_hour latest(date_mday) as date_mday latest(date_minute) as date_minute latest(date_wday) as date_wday latest(date_month) as date_month latest(date_second) as date_second latest(date_year) as date_year where index=* by index, sourcetype, host\
| join type=left sourcetype [rest splunk_server=local /servicesNS/-/-/configs/conf-props | search rename=* | stats count by title, rename  | fields + title rename | rename title as sourcetype]\
| eval host=lower(host) | eval index=lower(index) | eval sourcetype=trim(lower(if(isnotnull(rename),rename,sourcetype)), "\"") | `search_additions` | typer\
| stats max(lastTime) as lastTime list(eventtype) as eventtype by index, sourcetype, host\
| lookup expectedTime index,host,sourcetype OUTPUT | convert auto(suppressUntil) | eval contact=if(contact="",null(),contact)\
| fillnull value=`default_contact` contact | fillnull value=`default_expected_time` lateSecs\
| eval lateSecs=mvindex(lateSecs,0) | eval contact=mvindex(contact,0) | eval suppressUntil=mvindex(suppressUntil,0)\
| eval lateSecs=if(suppressUntil > now(),0,lateSecs) | eval lastAllowed=round(coalesce(relative_time(now(), lateSecs),now()-lateSecs),0)\
| where lateSecs != "0" AND ((lastTime < lastAllowed) OR (lastTime > now() + 3000)) AND (now() - lastTime < `ignore_after`) AND searchmatch("NOT eventtype=bh_suppress*")\
| eval howLateSecs=now() - lastTime | eval howLate=if(howLateSecs<0,"-".tostring(abs(howLateSecs),"duration"),tostring(howLateSecs,"duration"))\
| rex field=howLate mode=sed "s/\+/ days /" | rex field=howLate mode=sed "s/^1 days/1 day /"\
| eval suppressDate=strftime(suppressUntil,"%D %H:%M:%S") | sort lastTime | eval Last=strftime(lastTime, "%D %H:%M:%S")\
| eval message=index.", ".sourcetype.", ".host.", ".Last.", ".howLate."\n" | eval suppressfield=index.", ".sourcetype.", ".host\
| stats list(message) as Message, list(suppressfield) as suppressfield by contact\
| eval Message=mvappend("index, sourcetype, host, Time Of Last Data, Time Since Last Data\n",Message)

[Broken Hosts â€“ Suppress Until Is Set Past Date]
action.email = 1
action.email.inline = 1
action.email.message.alert = The alert condition for '$name$' was triggered.\
\
$result.Message$
action.email.sendresults = 1
action.email.to = $result.contact$
action.email.useNSSubject = 1
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = 1 0 * * *
description = Alerts when a suppressUntil value is configured to be suppressed until a date in the past. The following should be fixed.
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
enableSched = 1
quantity = 0
relation = greater than
search = | inputlookup expectedTime \
| eval suppressUntil_epoch=if(suppressUntil!="0", round(strptime(suppressUntil, "%m/%d/%Y %H:%M:%S"),0), 0)\
| where suppressUntil_epoch!="0" AND suppressUntil_epoch<now()

[Build State]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 0 0 * * *
dispatch.earliest_time = -1d
dispatch.latest_time = now
display.events.fields = ["host","source","sourcetype","bid{}","bugtraq","canvas_package","cert","cert-cc","cert{}","cpe","cpe{}","cve","cve{}","cvss","cvss3_base_score","cvss3_temporal_score","cvss3_temporal_vector","cvss3_vector","cvss_base_score","cvss_temporal_score","cvss_temporal_vector","cvss_vector","cwe","cwe{}","default_account","description","eventtype","exploit_available","exploit_framework_canvas","exploit_framework_core","exploit_framework_metasploit","exploitability_ease","exploited_by_malware","exploited_by_nessus","family_name","fedora","fname","glsa","id","index","linecount","mdksa","metasploit_name","msft","msft{}","mskb","osvdb","osvdb{}","patch_publication_date","plugin_modification_date","plugin_name","plugin_publication_date","plugin_type","potential_vulnerability","product","punct","rhsa","risk_factor","script_version","secunia","see_also","see_also{}","signature","signature_id","solution","splunk_server","synopsis","tag","tag::eventtype","timestamp","usn","vendor","vuln_publication_date","xref","xref{}","address","alias","current_state","currentDBSizeMB","dest","diff","display_name","execution_time","home","host_display_name","hostcheck_id","inode","latency","load_15_min","load_1_min","load_5_min","log","loss","maxTotalDataSizeMB","mem","mem_bytes","mem_free","mem_free_percent","mem_used","mem_used_bytes","opt","output","perfdata","pl","procs","ram_used","rta","RTA","service_display_name","servicecheck_id","size","splunk","swap","swap_bytes","swap_free","swap_free_percent","swap_used","swap_used_bytes","time","tmp","unsent","usr","var"]
display.general.type = statistics
display.page.search.mode = fast
display.page.search.tab = statistics
display.visualizations.charting.chart = line
enableSched = 1
quantity = 0
relation = greater than
request.ui_dispatch_app = search
request.ui_dispatch_view = search
search = | tstats count WHERE sourcetype=* earliest=-60d@d by _time host sourcetype index span=30m \
| makecontinuous _time span=30m \
| eval combo=host."-".sourcetype."-".index \
| xyseries _time combo count \
| makecontinuous _time span=30m \
| fillnull value=0 \
| untable _time combo count \
| eval HourOfDay=strftime(_time, "%H") \
| eval BucketMinuteOfHour=strftime(_time, "%M") \
| eval DayOfWeek=strftime(_time, "%A") \
| stats max(count) as avg stdev(count) as stdev by HourOfDay BucketMinuteOfHour DayOfWeek combo \
| eval lowerBound=(avg-stdev*exact(2)), upperBound=(avg+stdev*exact(2)) \
| fields lowerBound,upperBound,HourOfDay,BucketMinuteOfHour,DayOfWeek,combo \
| outputlookup state.csv

[Host Frequency]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 0 0 * * *
dispatch.earliest_time = -1d
dispatch.latest_time = now
enableSched = 1
quantity = 0
relation = greater than
search = | tstats count where index=* earliest=-60d@d latest=-30m@m latest=now  by host _indextime \
| bin _indextime span=30m \
| stats count by _indextime, host \
| stats count(host) as M by host \
| addinfo \
| eval N=round((info_max_time-info_min_time)/30/60,0)\
| eval frequency=M/N \
| fields host, frequency \
| sort - frequency | eventstats p95(frequency) as p95 p75(frequency) as p75 p50(frequency) as p50 p25(frequency) as p25\
| eval Percentile = case(frequency >= p95, "95",frequency >= p75, "75",frequency >= p50, "50", frequency >= p25, "25", 1=1, "0") | fields - p* | eval frequency=round(frequency*100,0)

[Index Frequency]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 0 0 * * *
dispatch.earliest_time = -1d
dispatch.latest_time = now
enableSched = 1
quantity = 0
relation = greater than
search = | tstats count where index=* earliest=-60d@d latest=-30m@m by index _indextime \
| bin _indextime span=30m \
| stats count by _indextime, index \
| stats count(index) as M by index \
| addinfo \
| eval N=round((info_max_time-info_min_time)/30/60,0)\
| eval frequency=M/N \
| fields index, frequency \
| sort - frequency | eventstats p95(frequency) as p95 p75(frequency) as p75 p50(frequency) as p50 p25(frequency) as p25\
| eval Percentile = case(frequency >= p95, "95",frequency >= p75, "75",frequency >= p50, "50", frequency >= p25, "25", 1=1, "0") | fields - p* | eval frequency=round(frequency*100,0)

[Sourcetype Frequency]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 0 0 * * *
dispatch.earliest_time = -1d
dispatch.latest_time = now
enableSched = 1
quantity = 0
relation = greater than
search = | tstats count where index=* earliest=-60d@d latest=-30m@m by sourcetype _indextime \
| bin _indextime span=30m \
| stats count by _indextime, sourcetype \
| stats count(sourcetype) as M by sourcetype \
| addinfo \
| eval N=round((info_max_time-info_min_time)/30/60,0)\
| eval frequency=M/N \
| fields sourcetype, frequency \
| sort - frequency | eventstats p95(frequency) as p95 p75(frequency) as p75 p50(frequency) as p50 p25(frequency) as p25\
| eval Percentile = case(frequency >= p95, "95",frequency >= p75, "75",frequency >= p50, "50", frequency >= p25, "25", 1=1, "0") | fields - p* | eval frequency=round(frequency*100,0)

[Index Data Anomalies Lookup Gen]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = 0 0 * * *
dispatch.earliest_time = -30d
dispatch.latest_time = now
enableSched = 1
quantity = 0
relation = greater than
search = index=_internal host=customer_host source=*license_usage.log* type="Usage" 
| bin _time span=15m
| stats sum(b) AS byte_sum by idx, _time
| xyseries _time, idx, byte_sum
| makecontinuous _time span=15m
| fillnull value=0
| untable _time idx byte_sum
| eval date_wday=strftime(_time,"%w")
| eval date_hour=strftime(_time,"%H")
| eval weekday_weekend=if(((date_wday > 0) AND (date_wday < 6)), "weekday", "weekend")
| eval biz_hours=if(((date_hour < 8) OR (date_hour > 18) OR (weekend_weekday=="weekend")), "no", "yes")
| stats avg(byte_sum) as average, stdev(byte_sum) as std by idx, weekday_weekend, biz_hours
| outputlookup client_baseline.csv

[Index Data Anomalies Alert]
alert.suppress = 0
alert.track = 1
counttype = number of events
cron_schedule = */15 * * * *
dispatch.earliest_time = -15m@m
dispatch.latest_time = now
enableSched = 1
quantity = 0
relation = greater than
search = index=_internal host=customer_host source=*license_usage.log* type="Usage" 
| bin _time span=15m 
| eval date_wday=strftime(_time,"%w")
| eval date_hour=strftime(_time,"%H")
| eval weekday_weekend=if(((date_wday > 0) AND (date_wday < 6)), "weekday", "weekend")
| eval biz_hours=if(((date_hour < 8) OR (date_hour > 18) OR (weekend_weekday=="weekend")), "no", "yes")
| stats sum(b) AS byte_sum by idx, weekday_weekend, biz_hours
| join type=inner idx,weekday_weekend, biz_hours [| inputlookup client_baseline.csv]
| eval devs=(byte_sum - average)/std
| eval currentGB=round((byte_sum/1024/1024/1024), 3)
| eval averageGB=round((average/1024/1024/1024), 3)
| table idx, currentGB, averageGB, devs
| where (devs > 2 OR devs < -2) AND averageGB > 0.05
| sort 0 - devs
| rename idx AS Index, currentGB AS "GB Indexed over Past 15 Minutes", averageGB AS "Average GB Indexed per 15 Minutes", devs AS "Z-Score"
